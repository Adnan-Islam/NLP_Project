{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import keras\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "comments = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('attack_annotations.tsv',  sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling Comment\n",
    "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label and Comment\n",
    "comments['attack'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Text\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.lower())\n",
    "comments['comment'] = comments['comment'].apply((lambda x: re.sub('.,[^a-zA-z0-9\\s]','',x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test sets\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "valid_comments = comments.query(\"split=='test'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation datasets \n",
    "train_x, valid_x = train_comments['comment'], valid_comments['comment'], \n",
    "train_y, valid_y = train_comments['attack'], valid_comments['attack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='\\\\w{1,}', tokenizer=None,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(comments['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(comments['comment'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(comments['comment'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(comments['comment'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\islam\\\\Desktop\\\\Anaconda'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('C:\\\\Users\\\\islam\\\\Desktop\\\\Anaconda\\\\wiki-news-300d-1M.vec', encoding=\"utf8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(comments['comment'])\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False, epochs=None):\n",
    "    \n",
    "    if is_neural_net:\n",
    "        classifier.fit(feature_vector_train, label ,epochs=epochs)\n",
    "        predictions = classifier.predict(feature_vector_valid)\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    else:\n",
    "        # Fit the training dataset on the classifier\n",
    "        classifier.fit(feature_vector_train, label)\n",
    "        # Predict the labels on validation dataset \n",
    "        predictions = classifier.predict(feature_vector_valid)\n",
    "        \n",
    "    accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "    f1score = metrics.f1_score(valid_y, predictions)\n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:   accuracy: 0.9291569591854345      f1 score: 0.6573455759599333\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"NB, Count Vectors:   accuracy: %s      f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, WordLevel TF-IDF:   accuracy: 0.933859694537924     f1 score: 0.631755945231804\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"NB, WordLevel TF-IDF:   accuracy: %s     f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, N-Gram Vectors:   accuracy: 0.9119855034946932     f1 score: 0.4785276073619632\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"NB, N-Gram Vectors:   accuracy: %s     f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, CharLevel Vectors:   accuracy: 0.9230304599188885   f1 score: 0.5806299952985425\n",
      "===============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"NB, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "print(\"===============================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\islam\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:   accuracy: 0.9294589697126585   f1 score: 0.6104360257326662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\islam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"LR, Count Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, WordLevel TF-IDF:   accuracy: 0.9418845456898783   f1 score: 0.6987251174233953\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"LR, WordLevel TF-IDF:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, N-Gram Vectors:   accuracy: 0.9138838553801019   f1 score: 0.48263348885432866\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"LR, N-Gram Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, CharLevel Vectors:   accuracy: 0.940676503580982   f1 score: 0.6917731450347454\n",
      "===============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"LR, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "print(\"===============================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Count Vectors:   accuracy: 0.9056432824229873   f1 score: 0.34891336707353376\n"
     ]
    }
   ],
   "source": [
    "# SVM Classifier on Count Vectors\n",
    "accuracy, f1score = train_model(svm.SVC(gamma='scale'), xtrain_count, train_y, xvalid_count)\n",
    "print(\"SVM, Count Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, WordLevel TF-IDF:   accuracy: 0.9433083096039347   f1 score: 0.7064343163538874\n"
     ]
    }
   ],
   "source": [
    "# SVM Classifier on Word Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(svm.SVC(gamma='scale'), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"SVM, WordLevel TF-IDF:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors TF-IDF:   accuracy: 0.9142721546293899   f1 score: 0.46999199786609763\n"
     ]
    }
   ],
   "source": [
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(svm.SVC(gamma='scale'), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"SVM, N-Gram Vectors TF-IDF:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, CharLevel Vectors:   accuracy: 0.9430925877987747   f1 score: 0.7007034263671432\n",
      "===============================================================================\n"
     ]
    }
   ],
   "source": [
    "# SVM Classifier on Character Level TF IDF Vectors\n",
    "accuracy, f1score = train_model(svm.SVC(gamma='scale'), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"SVM, CharLevel Vectors:   accuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "print(\"===============================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model\n",
    "def cnn(train_x, train_y, valid_x, batch_size=128, epochs = 1):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(train_x, train_y,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs)\n",
    "    \n",
    "    predictions = model.predict(valid_x)\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "    f1score = metrics.f1_score(valid_y, predictions) \n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "69526/69526 [==============================] - 115s 2ms/step - loss: 0.1644 - accuracy: 0.9397\n",
      "CNN, Word Embeddings:   acuuracy: 0.8810941409957719   f1 score: 0.0\n",
      "===============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\islam\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "accuracy, f1score = cnn(train_seq_x, train_y, valid_seq_x, 10)\n",
    "print(\"CNN, Word Embeddings:   acuuracy: %s   f1 score: %s\"% (accuracy,f1score))\n",
    "print(\"===============================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "def lstm(train_x, train_y, valid_x, batch_size=1024, epochs = 10):\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.LSTM(100))(embedding_layer)\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(train_x, train_y,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs)\n",
    "    \n",
    "    predictions = model.predict(valid_x)\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "    f1score = metrics.f1_score(valid_y, predictions) \n",
    "    return accuracy, f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "69526/69526 [==============================] - 347s 5ms/step - loss: 0.3102 - accuracy: 0.8911\n",
      "Epoch 2/10\n",
      "69526/69526 [==============================] - 353s 5ms/step - loss: 0.1757 - accuracy: 0.9349\n",
      "Epoch 3/10\n",
      "69526/69526 [==============================] - 351s 5ms/step - loss: 0.1610 - accuracy: 0.9404\n",
      "Epoch 4/10\n",
      "69526/69526 [==============================] - 352s 5ms/step - loss: 0.1536 - accuracy: 0.9434\n",
      "Epoch 5/10\n",
      "69526/69526 [==============================] - 347s 5ms/step - loss: 0.1512 - accuracy: 0.9435\n",
      "Epoch 6/10\n",
      "69526/69526 [==============================] - 352s 5ms/step - loss: 0.1482 - accuracy: 0.9448\n",
      "Epoch 7/10\n",
      "69526/69526 [==============================] - 351s 5ms/step - loss: 0.1461 - accuracy: 0.9460\n",
      "Epoch 8/10\n",
      "69526/69526 [==============================] - 351s 5ms/step - loss: 0.1433 - accuracy: 0.9459\n",
      "Epoch 9/10\n",
      "69526/69526 [==============================] - 346s 5ms/step - loss: 0.1412 - accuracy: 0.9470\n",
      "Epoch 10/10\n",
      "69526/69526 [==============================] - 349s 5ms/step - loss: 0.1375 - accuracy: 0.9472\n",
      "LSTM, Word Embeddings:  accuracy: 0.8810941409957719   f1 score: 0.0\n"
     ]
    }
   ],
   "source": [
    "accuracy, f1score = lstm(train_seq_x, train_y, valid_seq_x)\n",
    "print(\"LSTM, Word Embeddings:  accuracy: %s   f1 score: %s\"% (accuracy,f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
